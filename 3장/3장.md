# 3장

## BeautifulSoup4

### bs4 설치

- 주피터 노트북에 `pip install beautifulsoup4`
- `from bs4 import BeautifulSoup as bs` as쓰면 원하는 이름으로 import 가능

### soup.prettify()

- `prettify`를 쓰면 이렇게 html 구조를 파악하기 쉽게 바꿔준다.

### soup.children()

- 변수 한단계 아래에서 포함된 태그들을 알고 싶으면 `.children()`

- children으로 태그 조사

### soup.태그이름 

특정 태그안에 있는 데이터들을 뽑아내 준다.

### soup.태그이름.parent과 soup.태그이름.parent.name 

> ` parent` -> `children`으로 바꾸면 자식태그 사용

- soup.태그이름.parent은 특정 태그의 부모 태그의 내용을 알려준다.

- soup.태그이름.parent.name은 특정 태그의 부모 태그 이름을 알려준다.

#### soup.태그이름['class']

- 특정 태그의 class가 뭐라고 설정되어있는 지 알 수 있다.

#### soup.find('태그이름') 또는 soup.select_one('태그이름')

- 태그 이름에 해당하는 첫번째 태그를 뽑아준다.

#### soup.find_all('태그이름') 또는 soup.select('태그이름')

- 태그이름에 해당하는 모든 태그들을 뽑아준다.

#### soup.find와 soup.select의 차이

- `find`는 `html tag`를 통한 크롤링,  `select`는 `css`를 통한 크롤링에 쓰인다.

- 첫 번째 태그를 찾는 것은 `find, select_one `함수이고 모든 태그를 찾는 것은 `find_all, select `함수

- 가장 큰 차이점은 `select`는` > `로 하위태그를 쉽게 찾을 수 있다.

- 예를 들어`head `태그 아래있는` title`태그를 찾고 싶다고 할때, `select`가 더 직관적인 형태로 태그를 찾을 수 있다.

- `find_all - class_='outertext'`-> 첫 번째 태그 가져옴

```python
#find 
soup.find('div').find('p') 
#select 
soup.select_one('div > p')
```



#### soup.get_text()

- html에서 텍스트만 가져오는 메서드

### soup.tag .next_sibling과 previous_sibling

- 반복문으로 링크 가져오는 것 .string, each[href]가 링크만 가져오나

- `from urllib.request import urlopen # url 접근 함수`

## 실전

### urlib오류

- `[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed`오류

```python
import ssl
from bs4 import BeautifulSoup as bs
from urllib.request import urlopen

context = ssl._create_unverified_context()
# 의존성 추가
url_base = 'http://www.chicagomag.com'
url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'
url = url_base + url_sub

html = urlopen(url, context=context)
soup = bs(html, "html.parser")

soup
출처: https://multifrontgarden.tistory.com/219 [우리집앞마당]
```

- `https로 접근하기오류 403forbidden`

- 오류 발생이유 - urlib를 사용하면 서버 security가 봇을 감지해서 발생

```python
from bs4 import BeautifulSoup
from urllib.request import Request, urlopen


url_base = "https://www.chicagomag.com"
url_sub = "/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/"
url = url_base + url_sub

req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})
html = urlopen(req, context=context)
soup = BeautifulSoup(html, "html.parser")

soup
```

- 책 내용은 위 두개를 같이 적용해야지 오류가 안난다. 

### urlib 말고 request 쓰면 오류 안남

```python
import requests
from bs4 import BeautifulSoup

url = "https://movie.naver.com/movie/sdb/rank/rmovie.nhn?sel=cur&date=2021-12-12"
response = requests.get(url).text 
html = urlopen(request).read().decode()
soup_tmp = BeautifulSoup(html, "html.parser")
soup_tmp
```

### urllib - urljoin

- 절대경로로 잡힌 url은 그대로 두고 상대경로로 잡힌 url은 절대 경로로 변경가능

### '구분자'.join(리스트)

- join 함수는 매개변수로 들어온 리스트에 있는 요소 하나하나를 합쳐서 하나의 문자열로 바꾸어 반환하는 함수

### tqdm은  warning안뜨게 하는법

- `from tqdm import notebook`